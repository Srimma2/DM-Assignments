{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import re\n",
    "import copy #can delete\n",
    "\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
    "from sklearn.cross_validation import cross_val_score,cross_val_predict,KFold,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 30 #max number of sentences in a message\n",
    "MAX_NB_WORDS = 20000 #cap vocabulary\n",
    "GLOVE_FILE = 'glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "EMBEDDING_DIM = 200 #size of word vector \n",
    "TWITTER_FILE = 'training-Obama-Romney-tweets.xlsx'\n",
    "JAR_FILE = '/home/sreeraj/stanford-postagger-2016-10-31/stanford-postagger.jar'\n",
    "MODEL_FILE = '/home/sreeraj/stanford-postagger-2016-10-31/models/english-left3words-distsim.tagger'\n",
    "TOKENIZER = 'keras' #or use nltk\n",
    "STEMMER = 'wordnet' #or use snowball or porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Ytrue_Ypred(model,x,y):\n",
    "    #Y matrix is [1,0,0] for class 0, [0,1,0] for class 1, [0,0,1] for class -1\n",
    "    convert_to_label ={0:0,1:1,2:-1}\n",
    "    model_predictions = model.predict(x)\n",
    "    y_pred = np.zeros(len(y))\n",
    "    y_true = np.zeros(len(y))\n",
    "    #errors = 0.0\n",
    "    for i in range(len(y)):\n",
    "        y_pred[i] = convert_to_label[np.argmax(model_predictions[i])]\n",
    "        y_true[i] = convert_to_label[np.argmax(y[i])]\n",
    "        #if y_true[i] != y_pred[i]:\n",
    "            #errors+=1.0\n",
    "    return y_true,y_pred\n",
    "    \n",
    "\n",
    "# read the data\n",
    "obama_data = pd.read_excel(TWITTER_FILE,names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Obama')\n",
    "romney_data = pd.read_excel(TWITTER_FILE,names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Romney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date            time  \\\n",
      "0  2012-10-16 00:00:00  10:28:53-05:00   \n",
      "1  2012-10-16 00:00:00  10:04:30-05:00   \n",
      "2  2012-10-16 00:00:00  09:50:08-05:00   \n",
      "3  2012-10-16 00:00:00  10:00:16-05:00   \n",
      "4  2012-10-16 00:00:00  09:48:07-05:00   \n",
      "\n",
      "                                                text sentiment  \n",
      "0  Kirkpatrick, who wore a baseball cap embroider...         0  \n",
      "1  #<e>obama</e> debates that Cracker Ass Cracker...         1  \n",
      "2  @Hollivan @hereistheanswer  Youre missing the ...         0  \n",
      "3  I was raised as a Democrat  left the party yea...        -1  \n",
      "4  The <e>Obama camp</e> can't afford to lower ex...         0  \n",
      "                  date            time  \\\n",
      "0  2012-10-16 00:00:00  09:38:08-05:00   \n",
      "1  2012-10-16 00:00:00  10:14:18-05:00   \n",
      "2  2012-10-16 00:00:00  09:27:16-05:00   \n",
      "3  2012-10-16 00:00:00  10:11:43-05:00   \n",
      "4  2012-10-16 00:00:00  10:13:17-05:00   \n",
      "\n",
      "                                                text sentiment  \n",
      "0  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...        -1  \n",
      "1  .@WardBrenda @shortwave8669 @allanbourdius you...        -1  \n",
      "2  <e>Mitt Romney</e> still doesn't <a>believe</a...        -1  \n",
      "3  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...        -1  \n",
      "4  Hope <e>Romney</e> debate prepped w/ the same ...         1  \n"
     ]
    }
   ],
   "source": [
    "def get_data(data):\n",
    "    \"\"\" get and clean the data \"\"\"\n",
    "    data = data.iloc[1:]\n",
    "    data['text'] = data['text'].values.astype('unicode')\n",
    "    data['date'] = data['date'].values.astype('str')\n",
    "    data['time'] = data['time'].values.astype('unicode')\n",
    "    # remove rows with mixed sentiment\n",
    "    data = data[data['sentiment'] < 2]\n",
    "    data.index = range(len(data))\n",
    "    \n",
    "    return data\n",
    "\n",
    "obama_data = get_data(obama_data)\n",
    "romney_data = get_data(romney_data)\n",
    "\n",
    "obama_dataO = copy.deepcopy(obama_data)\n",
    "\n",
    "print obama_data.head()\n",
    "print romney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_dictionary = {':)':' smileyface ','(:':' smileyface ','XD': ' happyface ',':D': ' smileyface ','>.<':' smileyface ',':-)':' smileyface ',';)':' winkface ',';D':' winkface ',':\\'(':' cryingface '}\n",
    "\n",
    "emoticons = [':\\)','\\(:','XD',':D','>\\.<',':-\\)',';\\)',';D',':\\'\\(']\n",
    "\n",
    "emoticon_pattern = re.compile(r'(' + '\\s*|\\s*'.join(emoticons) + r')')\n",
    "\n",
    "# convert emoticons to words\n",
    "def emoticon_converter(x):\n",
    "    x = emoticon_pattern.sub(lambda i : emoticon_dictionary[i.group().replace(' ','')],x)   \n",
    "    return x\n",
    "\n",
    "obama_data['text'] = obama_data['text'].apply(emoticon_converter)\n",
    "romney_data['text'] = romney_data['text'].apply(emoticon_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_hashtag(x):\n",
    "    x = x.split()\n",
    "    temp = []\n",
    "    for word in x:\n",
    "        if '#' in word:\n",
    "            temp += re.findall('[A-Z][^A-Z]*',word)\n",
    "        else:\n",
    "            temp.append(word)\n",
    "    \n",
    "    return ' '.join(temp)\n",
    "\n",
    "obama_data['text'] = obama_data['text'].apply(separate_hashtag)\n",
    "romney_data['text'] = romney_data['text'].apply(separate_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "punc = ['\\:','\\;','\\?','\\$','\\.','\\(','\\)','\\#','\\=','\\%','\\-','\\>','\\<','\\,']\n",
    "cond_1 = re.compile('|'.join(punc))\n",
    "# remove tags\n",
    "tags = ['<a>','</a>','<e>','</e>']\n",
    "cond_2 = re.compile(\"|\".join(tags))\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\" preprocess the data\"\"\"\n",
    "     # remove users\n",
    "    data = data.apply(lambda x : re.sub(r'\\@\\s?\\w+','',x))\n",
    "    # remove hypertext \n",
    "    data = data.apply(lambda x : re.sub(r'http://\\S+','',x))\n",
    "    # remove tags\n",
    "    data = data.apply(lambda x : re.sub(cond_2,'',x))\n",
    "    # remove punctuations\n",
    "    data = data.apply(lambda x : re.sub(cond_1,'',x))\n",
    "    # remove digits\n",
    "    data = data.apply(lambda x : re.sub(r'[0-9]+','',x))\n",
    "    # convert to ascii\n",
    "    data = data.apply(lambda x: x.encode('utf-8'))\n",
    "    \n",
    "    return data\n",
    "\n",
    "obama_data['text'] = preprocess(obama_data['text'])\n",
    "romney_data['text'] = preprocess(romney_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## IMP - Process Emoticons, better stopwords list, clean hashtags\n",
    "# Tweet NLP\n",
    "\n",
    "manual_stopwords_list = ['RT']\n",
    "\n",
    "# stopwords list based on pos tags\n",
    "\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "jar = JAR_FILE\n",
    "model = MODEL_FILE\n",
    "\n",
    "st = StanfordPOSTagger(model, jar, encoding='utf8')\n",
    "\n",
    "remove_tags_pos = ['IN','DT','PRP','PRP$','WDT','WP','WP$','CD','PDT']\n",
    "remove_tags_tweetnlp = []\n",
    "\n",
    "\n",
    "def tweet_tag_filter(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "# obama_data['text'] = obama_data['text'].apply(tweet_tag_filter)\n",
    "# romney_data['text'] = romney_data['text'].apply(tweet_tag_filter)\n",
    "\n",
    "\n",
    "def pos_tag_filter(x):\n",
    "    x = x.split()\n",
    "    s = st.tag(x)\n",
    "    for i,(_,tag) in enumerate(s):\n",
    "        if tag in remove_tags_pos:\n",
    "            x[i] = ''\n",
    "    return ''.join(x)\n",
    "    \n",
    "\n",
    "# obama_data['text'] = obama_data['text'].apply(pos_tag_filter)\n",
    "# romney_data['text'] = romney_data['text'].apply(pos_tag_filter)\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "\n",
    "stopwords_list = stopwords.words('english') + manual_stopwords_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "class Tokenizer(object):\n",
    "    def __init__(self,stemmer='porter'):\n",
    "        self.stemmer = stemmer\n",
    "        if stemmer == 'wordnet':\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        if stemmer == 'porter':\n",
    "            self.wnl = PorterStemmer()\n",
    "        if stemmer == 'snowball':\n",
    "            self.wnl = SnowballStemmer('english')\n",
    "    def __call__(self, doc):\n",
    "        if self.stemmer == 'wordnet':\n",
    "            return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "        else:\n",
    "            return [self.wnl.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_y(data):\n",
    "    return data['text'],data['sentiment'].astype(int)\n",
    "\n",
    "texts = romney_data['text']\n",
    "labels = np.array(romney_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) #list of lists, basically replaces each word with number\n",
    "\n",
    "tokens = []\n",
    "myTokenizer = Tokenizer(STEMMER)\n",
    "for i in range(0,len(texts)):\n",
    "    try:\n",
    "        tokens.append(myTokenizer.__call__(texts[i]))\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "word_dict = {}\n",
    "winx = 1\n",
    "mysequences = []\n",
    "tsq = []\n",
    "for i in range(0,len(tokens)):\n",
    "    for token in tokens[i]:\n",
    "        if token not in word_dict:\n",
    "            word_dict[token] = winx\n",
    "            winx += 1\n",
    "        tsq.append(word_dict[token])\n",
    "    mysequences.append(tsq)\n",
    "    tsq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8662 unique tokens.\n",
      "[-1 -1 -1 -1]\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "('Shape of data tensor:', (5648, 30))\n",
      "('Shape of label tensor:', (5648, 3))\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index #key = word, value = number\n",
    "#word_index = word_dict\n",
    "#sequences = mysequences\n",
    "if TOKENIZER == 'nltk':\n",
    "    word_index = word_dict\n",
    "    sequences = mysequences\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#pad the data \n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print labels[0:4]\n",
    "Y = labels\n",
    "labels = keras.utils.np_utils.to_categorical(labels,nb_classes=3)\n",
    "print labels[0:4]\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(GLOVE_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#prepare embedding matrix\n",
    "\n",
    "#num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "def build_model():\n",
    "    np.random.seed(1)\n",
    "    l2 = regularizers.l2(0.01)\n",
    "    l22 = regularizers.l2(0.01)\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=1)\n",
    "    model.add(embedding_layer)\n",
    "    #model.add(LSTM(10,return_sequences=True))\n",
    "    model.add(LSTM(150,return_sequences=False,W_regularizer=l2))\n",
    "    #model.add(LSTM(15,return_sequences=False,W_regularizer=l22))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Dense(10, activation='relu'))\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    model.add(Dense(len(labels[0]), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.85      0.92      0.88       438\n",
      "        0.0       0.23      0.12      0.16        57\n",
      "        1.0       0.53      0.46      0.49        70\n",
      "\n",
      "avg / total       0.75      0.78      0.76       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.64      0.85      0.73       313\n",
      "        0.0       0.42      0.18      0.26       131\n",
      "        1.0       0.54      0.41      0.47       121\n",
      "\n",
      "avg / total       0.57      0.60      0.57       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.42      0.92      0.58       219\n",
      "        0.0       0.63      0.06      0.10       215\n",
      "        1.0       0.61      0.34      0.43       131\n",
      "\n",
      "avg / total       0.55      0.45      0.36       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.28      0.97      0.43       137\n",
      "        0.0       1.00      0.02      0.04       379\n",
      "        1.0       0.11      0.16      0.13        49\n",
      "\n",
      "avg / total       0.75      0.26      0.14       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.68      0.99      0.81       362\n",
      "        0.0       0.42      0.05      0.09       101\n",
      "        1.0       0.80      0.24      0.36       102\n",
      "\n",
      "avg / total       0.66      0.68      0.60       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.49      0.96      0.65       255\n",
      "        0.0       0.41      0.05      0.08       192\n",
      "        1.0       0.65      0.25      0.37       118\n",
      "\n",
      "avg / total       0.50      0.50      0.40       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.44      0.99      0.61       225\n",
      "        0.0       0.60      0.01      0.03       200\n",
      "        1.0       0.76      0.32      0.45       140\n",
      "\n",
      "avg / total       0.58      0.48      0.37       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.59      0.95      0.73       312\n",
      "        0.0       0.45      0.06      0.11       144\n",
      "        1.0       0.67      0.27      0.38       109\n",
      "\n",
      "avg / total       0.57      0.59      0.50       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.57      0.97      0.72       291\n",
      "        0.0       0.30      0.10      0.16       134\n",
      "        1.0       0.89      0.17      0.29       139\n",
      "\n",
      "avg / total       0.59      0.57      0.48       564\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.66      0.97      0.79       341\n",
      "        0.0       0.52      0.19      0.28       127\n",
      "        1.0       0.88      0.15      0.25        96\n",
      "\n",
      "avg / total       0.67      0.66      0.58       564\n",
      "\n",
      "Average f1-score =  0.39538578967\n"
     ]
    }
   ],
   "source": [
    "#k fold cross validaiton\n",
    "avg_acc = []\n",
    "avg_f1 = []\n",
    "\n",
    "#data = data[0:500]\n",
    "kf = KFold(n=len(data),n_folds=10)\n",
    "#kf = StratifiedKFold(Y,n_folds=10)\n",
    "for train,test in kf: #do the cross validation\n",
    "    np.random.seed(1)\n",
    "    x_train, x_val, y_train, y_val = data[train], data[test], labels[train], labels[test]\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(x_train, y_train, nb_epoch=10, batch_size=64,verbose=0)\n",
    "    y_true,y_pred = get_Ytrue_Ypred(model,x_val,y_val)\n",
    "    avg_acc.append(accuracy_score(y_true,y_pred))\n",
    "    avg_f1.append(f1_score(y_true,y_pred,average='macro'))      \n",
    "    print classification_report(y_true,y_pred)\n",
    "\n",
    "\n",
    "print classification_report(y_true,y_pred)\n",
    "print 'Average f1-score = ', np.mean(np.array(avg_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
