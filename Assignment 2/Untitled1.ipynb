{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import re\n",
    "import copy #can delete\n",
    "\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,f1_score,precision_score,recall_score\n",
    "from sklearn.cross_validation import cross_val_score,cross_val_predict,KFold,StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30 #max number of sentences in a message\n",
    "MAX_NB_WORDS = 20000 #cap vocabulary\n",
    "GLOVE_FILE = 'glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "EMBEDDING_DIM = 200 #size of word vector \n",
    "TWITTER_FILE = 'training-Obama-Romney-tweets.xlsx'\n",
    "JAR_FILE = '/home/sreeraj/stanford-postagger-2016-10-31/stanford-postagger.jar'\n",
    "MODEL_FILE = '/home/sreeraj/stanford-postagger-2016-10-31/models/english-left3words-distsim.tagger'\n",
    "TOKENIZER = 'keras' #or use nltk\n",
    "STEMMER = 'wordnet'\n",
    "\n",
    "\n",
    "def get_Ytrue_Ypred(model,x,y):\n",
    "    #Y matrix is [1,0,0] for class 0, [0,1,0] for class 1, [0,0,1] for class -1\n",
    "    convert_to_label ={0:0,1:1,2:-1}\n",
    "    model_predictions = model.predict(x)\n",
    "    y_pred = np.zeros(len(y))\n",
    "    y_true = np.zeros(len(y))\n",
    "    #errors = 0.0\n",
    "    for i in range(len(y)):\n",
    "        y_pred[i] = convert_to_label[np.argmax(model_predictions[i])]\n",
    "        y_true[i] = convert_to_label[np.argmax(y[i])]\n",
    "        #if y_true[i] != y_pred[i]:\n",
    "            #errors+=1.0\n",
    "    return y_true,y_pred\n",
    "    \n",
    "\n",
    "# read the data\n",
    "obama_data = pd.read_excel(TWITTER_FILE,names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Obama')\n",
    "romney_data = pd.read_excel(TWITTER_FILE,names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Romney')\n",
    "\n",
    "def get_data(data):\n",
    "    \"\"\" get and clean the data \"\"\"\n",
    "    data = data.iloc[1:]\n",
    "    data['text'] = data['text'].values.astype('unicode')\n",
    "    data['date'] = data['date'].values.astype('str')\n",
    "    data['time'] = data['time'].values.astype('unicode')\n",
    "    # remove rows with mixed sentiment\n",
    "    data = data[data['sentiment'] < 2]\n",
    "    data.index = range(len(data))\n",
    "    \n",
    "    return data\n",
    "\n",
    "obama_data = get_data(obama_data)\n",
    "romney_data = get_data(romney_data)\n",
    "\n",
    "obama_dataO = copy.deepcopy(obama_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_dictionary = {':)':' smileyface ','(:':' smileyface ','XD': ' happyface ',':D': ' smileyface ','>.<':' smileyface ',':-)':' smileyface ',';)':' winkface ',';D':' winkface ',':\\'(':' cryingface '}\n",
    "\n",
    "emoticons = [':\\)','\\(:','XD',':D','>\\.<',':-\\)',';\\)',';D',':\\'\\(']\n",
    "\n",
    "emoticon_pattern = re.compile(r'(' + '\\s*|\\s*'.join(emoticons) + r')')\n",
    "\n",
    "# convert emoticons to words\n",
    "def emoticon_converter(x):\n",
    "    x = emoticon_pattern.sub(lambda i : emoticon_dictionary[i.group().replace(' ','')],x)   \n",
    "    return x\n",
    "\n",
    "obama_data['text'] = obama_data['text'].apply(emoticon_converter)\n",
    "romney_data['text'] = romney_data['text'].apply(emoticon_converter)\n",
    "\n",
    "# http://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words\n",
    "# convert hashtags into words\n",
    "def separate_hashtag(x):\n",
    "    for i in range(0,len(x)):\n",
    "        hashtags = re.findall(r\"#(\\w+)\", x[i])\n",
    "        for words in hashtags:\n",
    "            x[i] = re.sub('#'+ words,split_hashtag(words.lower()),x[i])\n",
    "    return x\n",
    "\n",
    "#obama_data['text'] = separate_hashtag(obama_data['text'])\n",
    "#romney_data['text'] = separate_hashtag(romney_data['text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove punctuations\n",
    "punc = ['\\:','\\;','\\?','\\$','\\.','\\(','\\)','\\#',',','-','\\<','\\>','\\%','\\*']\n",
    "cond_1 = re.compile('|'.join(punc))\n",
    "# remove tags\n",
    "tags = ['<a>','</a>','<e>','</e>']\n",
    "cond_2 = re.compile(\"|\".join(tags))\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\" preprocess the data\"\"\"\n",
    "    # remove users\n",
    "    data = data.apply(lambda x : re.sub(r'\\@\\s?\\w+','',x))\n",
    "    # remove hypertext \n",
    "    data = data.apply(lambda x : re.sub(r\"http\\S+\",'',x)) #edited \n",
    "    # remove tags\n",
    "    data = data.apply(lambda x : re.sub(cond_2,'',x))\n",
    "    # remove punctuations\n",
    "    data = data.apply(lambda x : re.sub(cond_1,'',x))\n",
    "    # remove digits\n",
    "    data = data.apply(lambda x : re.sub(r'[0-9]+','',x))\n",
    "    # convert to ascii\n",
    "    data = data.apply(lambda x: x.encode('utf-8')) #sometimes doesn't work\n",
    "    # in that case\n",
    "    printable = set(string.printable)\n",
    "    for i in range(len(data)):\n",
    "        data[i] = filter(lambda x: x in printable, data[i])\n",
    "    \n",
    "    return data\n",
    "\n",
    "obama_data['text'] = preprocess(obama_data['text'])\n",
    "romney_data['text'] = preprocess(romney_data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manual_stopwords_list = ['RT','MT']\n",
    "\n",
    "\n",
    "stopwords_list = stopwords.words('english') + manual_stopwords_list\n",
    "\n",
    "# stemming\n",
    "class Tokenizer(object):\n",
    "    def __init__(self,stemmer='porter'):\n",
    "        self.stemmer = stemmer\n",
    "        if stemmer == 'wordnet':\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        if stemmer == 'porter':\n",
    "            self.wnl = PorterStemmer()\n",
    "        if stemmer == 'snowball':\n",
    "            self.wnl = SnowballStemmer('english')\n",
    "    def __call__(self, doc):\n",
    "        if self.stemmer == 'wordnet':\n",
    "            return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "        else:\n",
    "            return [self.wnl.stem(t) for t in word_tokenize(doc)]\n",
    "                \n",
    "            \n",
    "def get_X_y(data):\n",
    "    return data['text'],data['sentiment'].astype(int)\n",
    "\n",
    "\n",
    "texts = romney_data['text']\n",
    "labels = np.array(romney_data['sentiment'])\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) #list of lists, basically replaces each word with number\n",
    "\n",
    "tokens = []\n",
    "myTokenizer = Tokenizer(STEMMER)\n",
    "for i in range(0,len(texts)):\n",
    "    try:\n",
    "        tokens.append(myTokenizer.__call__(texts[i]))\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "word_dict = {}\n",
    "winx = 1\n",
    "mysequences = []\n",
    "tsq = []\n",
    "for i in range(0,len(tokens)):\n",
    "    for token in tokens[i]:\n",
    "        if token not in word_dict:\n",
    "            word_dict[token] = winx\n",
    "            winx += 1\n",
    "        tsq.append(word_dict[token])\n",
    "    mysequences.append(tsq)\n",
    "    tsq = []\n",
    "\n",
    "\n",
    "    \n",
    "word_index = tokenizer.word_index #key = word, value = number\n",
    "#word_index = word_dict\n",
    "#sequences = mysequences\n",
    "if TOKENIZER == 'nltk':\n",
    "    word_index = word_dict\n",
    "    sequences = mysequences\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#pad the data \n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# print labels[0:4]\n",
    "Y = labels\n",
    "labels = keras.utils.np_utils.to_categorical(labels,nb_classes=3)\n",
    "# print labels[0:4]\n",
    "\n",
    "# print('Shape of data tensor:', data.shape)\n",
    "# print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(GLOVE_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#prepare embedding matrix\n",
    "\n",
    "#num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "#create the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "np.random.seed(1)\n",
    "\n",
    "def build_model():\n",
    "    np.random.seed(1)\n",
    "    l2 = regularizers.l2(0.01)\n",
    "    l22 = regularizers.l2(0.01)\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=0)\n",
    "    model.add(embedding_layer)\n",
    "    #model.add(LSTM(10,return_sequences=True))\n",
    "    model.add(GRU(150,return_sequences=False,dropout_W=0.6,dropout_U=0.5)) #can also use LSTM\n",
    "    #model.add(LSTM(15,return_sequences=False,W_regularizer=l22))\n",
    "    #model.add(Dropout(0.2))\n",
    "#     model.add(Dense(30, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    model.add(Dense(len(labels[0]), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.61      0.95      0.74       290\n",
      "        0.0       0.61      0.21      0.32       168\n",
      "        1.0       0.72      0.35      0.47       108\n",
      "\n",
      "avg / total       0.63      0.62      0.56       566\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.56      0.87      0.68       290\n",
      "        0.0       0.44      0.14      0.22       168\n",
      "        1.0       0.56      0.31      0.40       108\n",
      "\n",
      "avg / total       0.52      0.55      0.49       566\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.58      0.92      0.71       290\n",
      "        0.0       0.55      0.16      0.25       168\n",
      "        1.0       0.53      0.26      0.35       108\n",
      "\n",
      "avg / total       0.56      0.57      0.50       566\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.61      0.90      0.72       289\n",
      "        0.0       0.54      0.27      0.36       168\n",
      "        1.0       0.41      0.21      0.28       108\n",
      "\n",
      "avg / total       0.55      0.58      0.53       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.62      0.94      0.75       289\n",
      "        0.0       0.41      0.18      0.25       168\n",
      "        1.0       0.58      0.30      0.39       108\n",
      "\n",
      "avg / total       0.55      0.59      0.53       565\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.65      0.81      0.72       289\n",
      "        0.0       0.54      0.42      0.47       168\n",
      "        1.0       0.65      0.44      0.53       107\n",
      "\n",
      "avg / total       0.62      0.62      0.61       564\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.64      0.89      0.75       289\n",
      "        0.0       0.53      0.20      0.29       168\n",
      "        1.0       0.59      0.58      0.58       107\n",
      "\n",
      "avg / total       0.60      0.62      0.58       564\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.58      0.88      0.70       289\n",
      "        0.0       0.38      0.11      0.17       168\n",
      "        1.0       0.48      0.33      0.39       107\n",
      "\n",
      "avg / total       0.50      0.55      0.48       564\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.57      0.82      0.68       289\n",
      "        0.0       0.41      0.18      0.25       168\n",
      "        1.0       0.66      0.46      0.54       107\n",
      "\n",
      "avg / total       0.54      0.56      0.52       564\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.69      0.85      0.76       289\n",
      "        0.0       0.59      0.46      0.52       168\n",
      "        1.0       0.71      0.48      0.57       107\n",
      "\n",
      "avg / total       0.66      0.66      0.65       564\n",
      "\n",
      "Average f1-score =  0.493454015829\n",
      "Overall Accuracy =  59.2439738268 %\n",
      "positive f1-score =  0.450557156238\n",
      "negative f1-score =  0.72007489097\n",
      "positive precision =  0.588839405412\n",
      "negative precision =  0.610031100692\n",
      "positive recall =  0.371555901696\n",
      "negative recall =  0.882442429304\n"
     ]
    }
   ],
   "source": [
    "#k fold cross validaiton\n",
    "avg_acc = []\n",
    "avg_f1 = []\n",
    "f_pos = []\n",
    "f_neg = []\n",
    "precision_pos = []\n",
    "precision_neg = []\n",
    "recall_pos = []\n",
    "recall_neg = []\n",
    "\n",
    "#data = data[0:500]\n",
    "# kf = KFold(n=len(data),n_folds=10)\n",
    "kf = StratifiedKFold(Y,n_folds=10)\n",
    "for train,test in kf: #do the cross validation\n",
    "    np.random.seed(1)\n",
    "    x_train, x_val, y_train, y_val = data[train], data[test], labels[train], labels[test]\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(x_train, y_train, nb_epoch=15, batch_size=64,verbose=0) #ep = 20 .5979\n",
    "    y_true,y_pred = get_Ytrue_Ypred(model,x_val,y_val)\n",
    "    avg_acc.append(accuracy_score(y_true,y_pred))\n",
    "    avg_f1.append(f1_score(y_true,y_pred,average='macro'))      \n",
    "    print classification_report(y_true,y_pred)\n",
    "    precision, recall, fscore, support = score(y_true, y_pred)\n",
    "    f_pos.append(fscore[2])\n",
    "    f_neg.append(fscore[0])\n",
    "    precision_pos.append(precision[2])\n",
    "    precision_neg.append(precision[0])\n",
    "    recall_pos.append(recall[2])\n",
    "    recall_neg.append(recall[0])\n",
    "    \n",
    "\n",
    "\n",
    "#print classification_report(y_true,y_pred)\n",
    "print 'Average f1-score = ', np.mean(np.array(avg_f1))\n",
    "print 'Overall Accuracy = ',100.0*np.mean(np.array(avg_acc)),'%'\n",
    "print 'positive f1-score = ', np.mean(np.array(f_pos))\n",
    "print 'negative f1-score = ', np.mean(np.array(f_neg))\n",
    "print 'positive precision = ', np.mean(np.array(precision_pos))\n",
    "print 'negative precision = ', np.mean(np.array(precision_neg))\n",
    "print 'positive recall = ', np.mean(np.array(recall_pos))\n",
    "print 'negative recall = ', np.mean(np.array(recall_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#get = word_index.items()\n",
    "#for i in range(0,len(get)):\n",
    "#    if get[i][1] == 6760:\n",
    "#        print get[i][0]\n",
    "#\n",
    "#np.random.seed(1)\n",
    "#model = build_model()\n",
    "#history = model.fit(data, labels, nb_epoch=70, validation_split=0.2, shuffle=True)\n",
    "#train_loss = history.history['loss']\n",
    "#val_loss = history.history['val_loss']\n",
    "#plt.plot(range(0,len(train_loss)),train_loss,label='train')\n",
    "#plt.plot(range(0,len(train_loss)),val_loss,label='val')\n",
    "#plt.legend(loc='left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
