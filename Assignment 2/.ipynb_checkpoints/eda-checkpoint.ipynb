{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date            time  \\\n",
      "0  2012-10-16 00:00:00  10:28:53-05:00   \n",
      "1  2012-10-16 00:00:00  10:04:30-05:00   \n",
      "2  2012-10-16 00:00:00  09:50:08-05:00   \n",
      "3  2012-10-16 00:00:00  10:00:16-05:00   \n",
      "4  2012-10-16 00:00:00  09:48:07-05:00   \n",
      "\n",
      "                                                                                                                                                 text  \\\n",
      "0            Kirkpatrick, who wore a baseball cap embroidered with Barack Obama's signature and had the hangdog look of Jason segel - Courier Journal   \n",
      "1                                                            #<e>obama</e> debates that Cracker Ass Cracker tonight... I will be tuned in! #TeamObama   \n",
      "2        @Hollivan @hereistheanswer  Youre missing the point  Im afraid you do not understand the bigger picture if you dont care if Obama re elected   \n",
      "3  I was raised as a Democrat  left the party years ago (1980)  but in my lifetime I have never seen a president divide the country like <e>obama</e>   \n",
      "4                        The <e>Obama camp</e> can't afford to lower expectations for tonight's debate. The President MUST give a strong performance.   \n",
      "\n",
      "  sentiment  \n",
      "0         0  \n",
      "1         1  \n",
      "2         0  \n",
      "3        -1  \n",
      "4         0  \n",
      "                  date            time  \\\n",
      "0  2012-10-16 00:00:00  09:38:08-05:00   \n",
      "1  2012-10-16 00:00:00  10:14:18-05:00   \n",
      "2  2012-10-16 00:00:00  09:27:16-05:00   \n",
      "3  2012-10-16 00:00:00  10:11:43-05:00   \n",
      "4  2012-10-16 00:00:00  10:13:17-05:00   \n",
      "\n",
      "                                                                                                                                                             text  \\\n",
      "0              Insidious!<e>Mitt Romney</e>'s Bain Helped Philip Morris Get U.S. High Schoolers <a>Hooked On Cigarettes</a> http://t.co/nMKuFcUq via @HuffPostPol   \n",
      "1                                                               .@WardBrenda @shortwave8669 @allanbourdius you mean like <e>romney </e><a>cheated in primary</a>?   \n",
      "2                                                                          <e>Mitt Romney</e> still doesn't <a>believe</a> that we <a>have a black president</a>.   \n",
      "3  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd look because he has a secret one that's different than the one <a>he's been lying about</a>http://t.co/arvfPQ7W   \n",
      "4                                                                                              Hope <e>Romney</e> debate prepped w/ the same people as last time.   \n",
      "\n",
      "  sentiment  \n",
      "0        -1  \n",
      "1        -1  \n",
      "2        -1  \n",
      "3        -1  \n",
      "4         1  \n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "obama_data = pd.read_excel('training-Obama-Romney-tweets.xlsx',names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Obama')\n",
    "romney_data = pd.read_excel('training-Obama-Romney-tweets.xlsx',names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Romney')\n",
    "\n",
    "def get_data(data):\n",
    "    \"\"\" get and clean the data \"\"\"\n",
    "    data = data.iloc[1:]\n",
    "    data['text'] = data['text'].values.astype('unicode')\n",
    "    data['date'] = data['date'].values.astype('str')\n",
    "    data['time'] = data['time'].values.astype('unicode')\n",
    "    # remove rows with mixed sentiment\n",
    "    data = data[data['sentiment'] < 2]\n",
    "    data.index = range(len(data))\n",
    "    \n",
    "    return data\n",
    "\n",
    "obama_data = get_data(obama_data)\n",
    "romney_data = get_data(romney_data)\n",
    "    \n",
    "print obama_data.head()\n",
    "print romney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5471 entries, 0 to 5470\n",
      "Data columns (total 4 columns):\n",
      "date         5471 non-null object\n",
      "time         5471 non-null object\n",
      "text         5471 non-null object\n",
      "sentiment    5471 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 213.7+ KB\n",
      "None\n",
      "                       date  time  \\\n",
      "count                  5471  5471   \n",
      "unique                   29  5139   \n",
      "top     2012-10-16 00:00:00   nan   \n",
      "freq                   1292     5   \n",
      "\n",
      "                                                       text  sentiment  \n",
      "count                                                  5471       5471  \n",
      "unique                                                 5453          3  \n",
      "top     I just knew it...<e>Obama</e> was born in Indonesia         -1  \n",
      "freq                                                      3       1922  \n"
     ]
    }
   ],
   "source": [
    "print obama_data.info()\n",
    "print obama_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5648 entries, 0 to 5647\n",
      "Data columns (total 4 columns):\n",
      "date         5648 non-null object\n",
      "time         5648 non-null object\n",
      "text         5648 non-null object\n",
      "sentiment    5648 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 220.6+ KB\n",
      "None\n",
      "                       date            time  \\\n",
      "count                  5648            5648   \n",
      "unique                   20            5342   \n",
      "top     2012-10-16 00:00:00  09:01:31-05:00   \n",
      "freq                   1713               4   \n",
      "\n",
      "                                                                                                                   text  \\\n",
      "count                                                                                                              5648   \n",
      "unique                                                                                                             5637   \n",
      "top     Ron Paul Won't Endorse <e>Romney</e>, Says More of Same - Yahoo! Finance http://t.co/7J00vBT6 via @YahooFinance   \n",
      "freq                                                                                                                  2   \n",
      "\n",
      "        sentiment  \n",
      "count        5648  \n",
      "unique          3  \n",
      "top            -1  \n",
      "freq         2893  \n"
     ]
    }
   ],
   "source": [
    "print romney_data.info()\n",
    "print romney_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoticon_dictionary = {':)':' smileyface ','(:':' smileyface ','XD': ' happyface ',':D': ' smileyface ','>.<':' smileyface ',':-)':' smileyface ',';)':' winkface ',';D':' winkface ',':\\'(':' cryingface '}\n",
    "\n",
    "emoticons = [':\\)','\\(:','XD',':D','>\\.<',':-\\)',';\\)',';D',':\\'\\(']\n",
    "\n",
    "emoticon_pattern = re.compile(r'(' + '\\s*|\\s*'.join(emoticons) + r')')\n",
    "\n",
    "# convert emoticons to words\n",
    "def emoticon_converter(x):\n",
    "    x = emoticon_pattern.sub(lambda i : emoticon_dictionary[i.group().replace(' ','')],x)   \n",
    "    return x\n",
    "\n",
    "obama_data['text'] = obama_data['text'].apply(emoticon_converter)\n",
    "romney_data['text'] = romney_data['text'].apply(emoticon_converter)\n",
    "\n",
    "# http://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words\n",
    "# convert hashtags into words (based on capitals)\n",
    "def separate_hashtag(x):\n",
    "    x = x.split()\n",
    "    temp = []\n",
    "    for i,word in enumerate(x):\n",
    "        if '#' in word:\n",
    "            temp += re.findall('[A-Z][^A-Z]*',word)\n",
    "        else:\n",
    "            temp.append(word)\n",
    "    \n",
    "    return ' '.join(temp)\n",
    "\n",
    "obama_data['text'] = obama_data['text'].apply(separate_hashtag)\n",
    "romney_data['text'] = romney_data['text'].apply(separate_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exclamation is imp; don't remove\n",
    "# remove punctuations\n",
    "punc = ['\\:','\\;','\\?','\\$','\\.','\\(','\\)','\\#','\\=','\\%','\\-','\\>','\\<','\\,','\\\"','\\\\','\\&']\n",
    "cond_1 = re.compile('|'.join(punc))\n",
    "# remove tags\n",
    "tags = ['<a>','</a>','<e>','</e>']\n",
    "cond_2 = re.compile(\"|\".join(tags))\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\" preprocess the data\"\"\"\n",
    "     # remove users\n",
    "    data = data.apply(lambda x : re.sub(r'\\@\\s?\\w+','',x))\n",
    "    # remove hypertext \n",
    "    data = data.apply(lambda x : re.sub(r'http://\\S+','',x))\n",
    "    # remove tags\n",
    "    data = data.apply(lambda x : re.sub(cond_2,'',x))\n",
    "    # remove punctuations\n",
    "    data = data.apply(lambda x : re.sub(cond_1,'',x))\n",
    "    # remove digits\n",
    "    data = data.apply(lambda x : re.sub(r'[0-9]+','',x))\n",
    "    # convert to ascii\n",
    "    data = data.apply(lambda x: x.encode('utf-8'))\n",
    "    \n",
    "    return data\n",
    "\n",
    "obama_data['text'] = preprocess(obama_data['text'])\n",
    "romney_data['text'] = preprocess(romney_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          Kirkpatrick who wore a baseball cap embroidered with Barack Obama's signature and had the hangdog look of Jason segel  Courier Journal\n",
      "1                                                                         debates that Cracker Ass Cracker tonight I will be tuned in! Team Obama\n",
      "2                                 Youre missing the point Im afraid you do not understand the bigger picture if you dont care if Obama re elected\n",
      "3             I was raised as a Democrat left the party years ago  but in my lifetime I have never seen a president divide the country like obama\n",
      "4                             The Obama camp can't afford to lower expectations for tonight's debate The President MUST give a strong performance\n",
      "5                                                                                         Obama pot policy disappointing  to say the least!  via \n",
      "6                                       Not all of Hollywood has his back! RT  Gene Simmons Yanks Obama Support Calls Him a “PissPoor” President \n",
      "7                                                               Obama's Expedient Speak fair in order to Slenderizing BiscuitTaiwan Tensions GjN \n",
      "8                                                                                                  I had a dream that i was smoking with Obama oO\n",
      "9                                         The Washington Times The President’s Popularity Bubble Has Burst  Barack Obama  Fox Nation  Obama O F A\n",
      "10                                                                 Obama's priorities K for gaydar research not security for our embassy in Libya\n",
      "11                                                                               United states of Islam  Does Obama really think that Weirdoooooo\n",
      "12                                         they created this debt now plame Obama handing out money to billionaires and two wars u braked u owned\n",
      "13                                                     Obama has not kept ONE of his  promises of the  town hall debates Why believe him tonight \n",
      "14                                                                                                      This sub is here talking shit about Obama\n",
      "15                                                                                                                                  obama ist tot\n",
      "16                     when you go against your own ideas to hurt Obama and America for electing a blk pres You've shown your whole hand dumbass!\n",
      "17                                                                                                               Still my Idol Mr President Obama\n",
      "18                                       Pretty creepy! No wonder Obama always has Starbursts in his pocket Some equally realistic theories here \n",
      "19                                                                              Just saw a truck with a NASA sticker and an Obama sticker New BFF\n",
      "20                                                             debate tonight! I hope Obama brings a lot of energy tonight This debate is crucial\n",
      "21                                                           you said so much good things about Obama when the small business owner picked him up\n",
      "22      Hillary taken the spaken for Obama's ignorance Attaboy Obama! The more blind people that have ur back the better off u will be Spineless!\n",
      "23                                                                     I'm a South African and i say Obama is the best thing to happen to America\n",
      "24                                 RT  Smart women know when Obama said his policies would cause energy prices to skyrocket he meant it! // S G P\n",
      "25                  Tonight Obama will be focused and forceful! Leaving voters to wonder why he didn't know he should do that in the first debate\n",
      "26                                                  Obama may need ground game of his backers are following elex very closely compared with  in  \n",
      "27                                                    i remember when Obama first got elected niggas acted like it was the second coming of Jesus\n",
      "28                                                                                                                Obama is working out next to me\n",
      "29                                                     Pretty much RT   If Obama loses tonight expect the madness meltdown to breach containment \n",
      "                                                                          ...                                                                    \n",
      "5441                                                                                    Obama urged to cut ties with muslim Brotherhood | Times  \n",
      "5442                                                                                             Obama ‰ÛÏganador‰Û ‰ÛÒ TijuanaHOY ‰ÛÒ Noticias \n",
      "5443                                         Always an awesome sight RT  Satellite trucks are lining up in preparation for Obama's speech  today \n",
      "5444                It's great that Obama came to OU but they have everything blocked off and it makes getting to class on time nearly impossible\n",
      "5445                                                                                                            Coal Miners declare war on Obama \n",
      "5446                       Except Brown was an arse! RT  Obama seems a serious politician who can't be bothered with silly games a bit like Brown\n",
      "5447                                            THIS   Obama nd term agenda MT  Tampa Bay Times Obama did little  lay out an agenda for a nd term\n",
      "5448                                                                                                                 Obama  A cult of personality\n",
      "5449                                                                                                                   I Dont Have Time For Obama\n",
      "5450                                                                                                                            Obama definitely \n",
      "5451                                                                                                  Michelle Obama Hey Whitey comment  YouTube \n",
      "5452                                                                              The Coup  Obama est la marionnette de la classe dominante  via \n",
      "5453                                                                 Barack Obama Singing Sexy and I Know It by LMFAO  I love all of these videos\n",
      "5454                                                                                                  Obama campaigns for Clinton‰Ûªs third term \n",
      "5455                                                                   Best statement by a focus group member Obama is bullshitting on everything\n",
      "5456            Obama should start challenging leaders to dunk contests Like Hey China double or nothing on the national debt Boom Economy solved\n",
      "5457                                                                                                                  Obama's Business Experience\n",
      "5458                         When Candy said Obama wants to be seen as successful Elizabeth had that weird satisfied look on her face She's funny\n",
      "5459      But BUT‰Û_ It is a mistake to assume that Women are only always influenced by just women's issuesHence Obama's phrase pocket book issue\n",
      "5460                                            MT RT  Guy from town hall debate says Obama didn't really answer his question on Libya security  \n",
      "5461                                                                  MittGangnam Style was funnier but just for balance Barack Obama Style  via \n",
      "5462                                                                                       Yo look at some of the shit Obama signed into law tho \n",
      "5463                           There is and always has been a glass ceiling for women in the workforce Don't fool yourself Obama won't change it \n",
      "5464                                                                 ‰ÛÏ When they for Obama who kan be against him Skr Up  in the back killin me\n",
      "5465                                                                                        When Jacobs blames Obama for her computer not working\n",
      "5466                                                  except for women who work in the WH they make  less honest!  Yup  Obama  supports Equal Pay\n",
      "5467                                           Days to Election & Selection Elect Lewis Ken Hall  Nation Re Elect president Obama Stay the Course\n",
      "5468                             The Reason Ann Romney And Michelle Obama Matched Last Night Michelle Obama and Ann Romney showed up to last nig \n",
      "5469                                                                                                     Obama Kenakan Cincin Syahadat Sejak SMA \n",
      "5470                                                                                Bitches be like Obama bitches just want food stamps lmao _Ù÷â\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print obama_data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_time(data):\n",
    "    \"\"\" processes time \"\"\"\n",
    "\n",
    "    def extract_date(pattern,string):\n",
    "        temp = re.match(pattern,string)\n",
    "        if temp:\n",
    "            return temp.group(1)\n",
    "        else:\n",
    "            return string\n",
    "    # clean date\n",
    "    date_format_1 = re.compile('\\d+/(\\d{2})/\\d+')\n",
    "    date_format_2 = re.compile('\\d+\\-\\d+\\-(\\d{2})')\n",
    "    date_format_3 = re.compile('(\\d{2})\\-[a-zA-Z]+\\-\\d+')\n",
    "    date_format = [date_format_1] + [date_format_2] + [date_format_3]\n",
    "\n",
    "    # remove whitespace\n",
    "    data['date'] = data['date'].apply(lambda x : x.replace(' ',''))\n",
    "\n",
    "    for i in date_format:\n",
    "        data['date'] = data['date'].apply(lambda x: extract_date(i,x))\n",
    "\n",
    "    def converter(first,second):\n",
    "        if first == 'AM':\n",
    "            return second\n",
    "        else:\n",
    "            val = re.findall('(\\d{1,2})',second)[0]\n",
    "            if int(val) > 12:\n",
    "                val = str(int(val) + 12)\n",
    "            return re.sub('\\d{1,2}',val,second,1)\n",
    "\n",
    "    def extract_time(pattern,string):\n",
    "\n",
    "        temp = re.match(pattern,string)\n",
    "        if temp:\n",
    "            first = temp.group(1)\n",
    "            second = temp.group(2)\n",
    "            third = temp.group(3)\n",
    "\n",
    "            if first is None and third is None:\n",
    "                return second\n",
    "\n",
    "            if first == 'AM' or first == 'PM':\n",
    "                return converter(first,second)\n",
    "            else:\n",
    "                return converter(third,second)\n",
    "\n",
    "    # clean time\n",
    "    time_format_1 = re.compile('(AM|PM)?\\s?(\\d{1,2}:\\d{1,2}:\\d{1,2})\\s?(AM|PM)?')\n",
    "\n",
    "    # remove whitespace\n",
    "    data['time'] = data['time'].apply(lambda x : x.replace(' ',''))\n",
    "\n",
    "    data['time'] = data['time'].apply(lambda x : extract_time(time_format_1,x))\n",
    "    data['time'] = pd.to_datetime(data['time'], format='%H:%M:%S')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    1922\n",
      " 0    1896\n",
      " 1    1653\n",
      "Name: sentiment, dtype: int64\n",
      "-1    2893\n",
      " 0    1680\n",
      " 1    1075\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Container object of 10 artists>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAE8CAYAAACb07NnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQEEJPgAAgIL6fwrEXRcph4q0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18cc5170d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of sentiment\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "obama_heights,obama_bins = np.histogram(obama_data['sentiment'])\n",
    "romney_heights,romney_bins = np.histogram(romney_data['sentiment'],bins = obama_bins)\n",
    "\n",
    "print obama_data['sentiment'].value_counts()\n",
    "print romney_data['sentiment'].value_counts()\n",
    "\n",
    "width = (obama_bins[1] - obama_bins[0])/3\n",
    "\n",
    "ax.bar(obama_bins[:-1],obama_heights,width = width,color = 'blue')\n",
    "ax.bar(romney_bins[:-1]+width,romney_heights,width = width, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## IMP - Process Emoticons, better stopwords list, clean hashtags\n",
    "# Tweet NLP\n",
    "\n",
    "import nltk\n",
    "\n",
    "manual_stopwords_list = ['RT','MT']\n",
    "\n",
    "# stopwords list based on pos tags\n",
    "\n",
    "remove_tags_nltkpos = ['IN','DT','PRP','CC']\n",
    "\n",
    "\n",
    "def pos_tag_filter(x):\n",
    "    x = x.split()\n",
    "    s = nltk.pos_tag(x)\n",
    "    for i,(_,tag) in enumerate(s):\n",
    "        if tag in remove_tags_nltkpos:\n",
    "            x[i] = ''\n",
    "    return ' '.join(x)\n",
    "    \n",
    "\n",
    "# obama_data['text'] = obama_data['text'].apply(pos_tag_filter)\n",
    "romney_data['text'] = romney_data['text'].apply(pos_tag_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obama_data['text'] = obama_data['text'].apply(lambda x : x.lower())\n",
    "romney_data['text'] = romney_data['text'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     kirkpatrick who wore a baseball cap embroidered with barack obama's signature and had the hangdog look of jason segel  courier journal\n",
      "1                                                                    debates that cracker ass cracker tonight i will be tuned in! team obama\n",
      "2                            youre missing the point im afraid you do not understand the bigger picture if you dont care if obama re elected\n",
      "3        i was raised as a democrat left the party years ago  but in my lifetime i have never seen a president divide the country like obama\n",
      "4                        the obama camp can't afford to lower expectations for tonight's debate the president must give a strong performance\n",
      "5                                                                                    obama pot policy disappointing  to say the least!  via \n",
      "6                                  not all of hollywood has his back! rt  gene simmons yanks obama support calls him a “pisspoor” president \n",
      "7                                                          obama's expedient speak fair in order to slenderizing biscuittaiwan tensions gjn \n",
      "8                                                                                             i had a dream that i was smoking with obama oo\n",
      "9                                    the washington times the president’s popularity bubble has burst  barack obama  fox nation  obama o f a\n",
      "10                                                            obama's priorities k for gaydar research not security for our embassy in libya\n",
      "11                                                                          united states of islam  does obama really think that weirdoooooo\n",
      "12                                    they created this debt now plame obama handing out money to billionaires and two wars u braked u owned\n",
      "13                                                obama has not kept one of his  promises of the  town hall debates why believe him tonight \n",
      "14                                                                                                 this sub is here talking shit about obama\n",
      "Name: text, dtype: object\n",
      "0                             insidious!mitt romney's bain helped philip morris get  high schoolers hooked cigarettes\n",
      "1                                                                                         mean romney cheated primary\n",
      "2                                                              mitt romney still doesn't believe have black president\n",
      "3                              romney's tax plan deserves nd look has secret one that's different one he's been lying\n",
      "4                                                                 hope romney debate prepped w/ same people last time\n",
      "5                 want to know how mitt romney is going to be able to cut trillon dollars go here explains everything\n",
      "6                                                         romney wins presidential election worlds really ending year\n",
      "7                                       romney's million jobs scam reminds rip torn selling pennies movies sick puppy\n",
      "8                 mitt romney said catching osama bin laden would be insignificant it's not worth moving heaven earth\n",
      "9                                              please be mitt romney “ who is honey boo boo endorsing president video\n",
      "10                                                          romney is disappointed when states allow samesex marriage\n",
      "11                  women polls show are truly voting romney then don't want to know how truly take care  household's\n",
      "12                                when comes  trillion tax plan don't let mitt romney give u runaround details matter\n",
      "13                            mt | reminder mitt romney isn't changing positions he's refusing to admit positions has\n",
      "14    ppl know romney is liar hides  taxes makes false accusations agnst women rights yet buy cant bleivemakesnosense\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print obama_data['text'][:15]\n",
    "print romney_data['text'][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english') + manual_stopwords_list\n",
    "\n",
    "# stemming\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self,stemmer='porter'):\n",
    "        self.stemmer = stemmer\n",
    "        if stemmer == 'wordnet':\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        if stemmer == 'porter':\n",
    "            self.wnl = PorterStemmer()\n",
    "        if stemmer == 'snowball':\n",
    "            self.wnl = SnowballStemmer('english')\n",
    "    def __call__(self, doc):\n",
    "        if self.stemmer == 'wordnet':\n",
    "            return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "        else:\n",
    "            return [self.wnl.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5471,) (5471,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def get_X_y(data):\n",
    "    return data['text'],data['sentiment'].astype(int)\n",
    "\n",
    "X,y = get_X_y(obama_data)\n",
    "print X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "text_vector = Pipeline([('vect', CountVectorizer(tokenizer = Tokenizer('wordnet'),stop_words = [],ngram_range = (1,2),max_features=10000)),\n",
    "                    ('tfidf',TfidfTransformer())])\n",
    "svd_transform = TruncatedSVD(n_components = 1000,n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform the data\n",
    "X = text_vector.fit_transform(X)\n",
    "X_reduced = svd_transform.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
    "\n",
    "classifier_scores = dict()\n",
    "\n",
    "def naive_classifier():\n",
    "    return 'Naive_Bayes',MultinomialNB()\n",
    "\n",
    "def svm_classifier():\n",
    "    return 'Linear_SVM',LinearSVC()\n",
    "\n",
    "def sgd_classifier():\n",
    "    return 'SGD',SGDClassifier()\n",
    "\n",
    "classifiers_list = [naive_classifier(),svm_classifier(),sgd_classifier()]\n",
    "\n",
    "for clf_name,clf in classifiers_list:\n",
    "    # dont use reduced matrix for naive bayes\n",
    "    if clf_name != 'Naive_Bayes':\n",
    "            X = X_reduced\n",
    "    classifier_scores[clf_name] = dict()\n",
    "    classifier_scores[clf_name]['classification_pred'] = cross_val_predict(clf,X,y,cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier - Naive_Bayes\n",
      "accuracy is 0.592761835131\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.56      0.72      0.63      1922\n",
      "          0       0.57      0.49      0.53      1896\n",
      "          1       0.68      0.56      0.61      1653\n",
      "\n",
      "avg / total       0.60      0.59      0.59      5471\n",
      "\n",
      "Classifier - Linear_SVM\n",
      "accuracy is 0.581977700603\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.58      0.61      0.60      1922\n",
      "          0       0.54      0.54      0.54      1896\n",
      "          1       0.63      0.60      0.62      1653\n",
      "\n",
      "avg / total       0.58      0.58      0.58      5471\n",
      "\n",
      "Classifier - SGD\n",
      "accuracy is 0.580332663133\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.60      0.60      0.60      1922\n",
      "          0       0.54      0.53      0.53      1896\n",
      "          1       0.60      0.62      0.61      1653\n",
      "\n",
      "avg / total       0.58      0.58      0.58      5471\n",
      "\n"
     ]
    }
   ],
   "source": [
    " for clf_name,_ in classifiers_list:\n",
    "        print 'Classifier - {}'.format(clf_name)\n",
    "        print 'accuracy is {}'.format(accuracy_score(y,classifier_scores[clf_name]['classification_pred']))\n",
    "        print classification_report(y,classifier_scores[clf_name]['classification_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Romney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5648,) (5648,)\n"
     ]
    }
   ],
   "source": [
    "X,y = get_X_y(romney_data)\n",
    "print X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "text_vector = Pipeline([('vect', CountVectorizer(tokenizer = Tokenizer('wordnet'),stop_words = stopwords_list,ngram_range = (1,2),max_features=10000)),\n",
    "                    ('tfidf',TfidfTransformer())])\n",
    "svd_transform = TruncatedSVD(n_components = 1000,n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the data\n",
    "X = text_vector.fit_transform(X)\n",
    "X_reduced = svd_transform.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier_scores = dict()\n",
    "\n",
    "def naive_classifier():\n",
    "    return 'Naive_Bayes',MultinomialNB()\n",
    "\n",
    "def svm_classifier():\n",
    "    return 'Linear_SVM',LinearSVC(class_weight = 'balanced')\n",
    "\n",
    "def sgd_classifier():\n",
    "    return 'SGD',SGDClassifier()\n",
    "\n",
    "classifiers_list = [naive_classifier(),svm_classifier(),sgd_classifier()]\n",
    "\n",
    "for clf_name,clf in classifiers_list:\n",
    "    # dont use reduced matrix for naive bayes\n",
    "    if clf_name != 'Naive_Bayes':\n",
    "            X = X_reduced\n",
    "    classifier_scores[clf_name] = dict()\n",
    "    classifier_scores[clf_name]['classification_pred'] = cross_val_predict(clf,X,y,cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier - Naive_Bayes\n",
      "accuracy is 0.549752124646\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.55      0.94      0.70      2893\n",
      "          0       0.47      0.15      0.23      1680\n",
      "          1       0.65      0.13      0.22      1075\n",
      "\n",
      "avg / total       0.55      0.55      0.47      5648\n",
      "\n",
      "Classifier - Linear_SVM\n",
      "accuracy is 0.52230878187\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.64      0.63      0.63      2893\n",
      "          0       0.39      0.38      0.38      1680\n",
      "          1       0.42      0.47      0.44      1075\n",
      "\n",
      "avg / total       0.53      0.52      0.52      5648\n",
      "\n",
      "Classifier - SGD\n",
      "accuracy is 0.546211048159\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.63      0.73      0.67      2893\n",
      "          0       0.42      0.28      0.33      1680\n",
      "          1       0.43      0.47      0.45      1075\n",
      "\n",
      "avg / total       0.53      0.55      0.53      5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    " for clf_name,_ in classifiers_list:\n",
    "        print 'Classifier - {}'.format(clf_name)\n",
    "        print 'accuracy is {}'.format(accuracy_score(y,classifier_scores[clf_name]['classification_pred']))\n",
    "        print classification_report(y,classifier_scores[clf_name]['classification_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.cross_validation import KFold,StratifiedKFold\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30 #max number of sentences in a message\n",
    "MAX_NB_WORDS = 20000 #cap vocabulary\n",
    "GLOVE_FILE = 'glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "EMBEDDING_DIM = 200 #size of word vector \n",
    "TWITTER_FILE = 'training-Obama-Romney-tweets.xlsx'\n",
    "JAR_FILE = '/home/sreeraj/stanford-postagger-2016-10-31/stanford-postagger.jar'\n",
    "MODEL_FILE = '/home/sreeraj/stanford-postagger-2016-10-31/models/english-left3words-distsim.tagger'\n",
    "TOKENIZER = 'keras' #or use nltk\n",
    "STEMMER = 'wordnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_Ytrue_Ypred(model,x,y):\n",
    "    #Y matrix is [1,0,0] for class 0, [0,1,0] for class 1, [0,0,1] for class -1\n",
    "    convert_to_label ={0:0,1:1,2:-1}\n",
    "    model_predictions = model.predict(x)\n",
    "    y_pred = np.zeros(len(y))\n",
    "    y_true = np.zeros(len(y))\n",
    "    #errors = 0.0\n",
    "    for i in range(len(y)):\n",
    "        y_pred[i] = convert_to_label[np.argmax(model_predictions[i])]\n",
    "        y_true[i] = convert_to_label[np.argmax(y[i])]\n",
    "        #if y_true[i] != y_pred[i]:\n",
    "            #errors+=1.0\n",
    "    return y_true,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = romney_data['text']\n",
    "labels = np.array(romney_data['sentiment'])\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) #list of lists, basically replaces each word with number\n",
    "\n",
    "tokens = []\n",
    "myTokenizer = Tokenizer(STEMMER)\n",
    "for i in range(0,len(texts)):\n",
    "    try:\n",
    "        tokens.append(myTokenizer.__call__(texts[i]))\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "word_dict = {}\n",
    "winx = 1\n",
    "mysequences = []\n",
    "tsq = []\n",
    "for i in range(0,len(tokens)):\n",
    "    for token in tokens[i]:\n",
    "        if token not in word_dict:\n",
    "            word_dict[token] = winx\n",
    "            winx += 1\n",
    "        tsq.append(word_dict[token])\n",
    "    mysequences.append(tsq)\n",
    "    tsq = []\n",
    "\n",
    "\n",
    "    \n",
    "word_index = tokenizer.word_index #key = word, value = number\n",
    "#word_index = word_dict\n",
    "#sequences = mysequences\n",
    "if TOKENIZER == 'nltk':\n",
    "    word_index = word_dict\n",
    "    sequences = mysequences\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#pad the data \n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# print labels[0:4]\n",
    "Y = labels\n",
    "labels = keras.utils.np_utils.to_categorical(labels,nb_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(GLOVE_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#prepare embedding matrix\n",
    "\n",
    "#num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "np.random.seed(1)\n",
    "\n",
    "def build_model():\n",
    "    np.random.seed(1)\n",
    "    l2 = regularizers.l2(0.01)\n",
    "    l22 = regularizers.l2(0.01)\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=0)\n",
    "    model.add(embedding_layer)\n",
    "    #model.add(LSTM(10,return_sequences=True))\n",
    "    model.add(GRU(150,return_sequences=False,dropout_W=0.6,dropout_U=0.5)) #can also use LSTM\n",
    "    #model.add(LSTM(15,return_sequences=False,W_regularizer=l22))\n",
    "    #model.add(Dropout(0.2))\n",
    "#     model.add(Dense(30, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    model.add(Dense(len(labels[0]), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.57      0.95      0.71      1447\n",
      "        0.0       0.56      0.15      0.24       840\n",
      "        1.0       0.57      0.18      0.28       538\n",
      "\n",
      "avg / total       0.57      0.57      0.49      2825\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.59      0.88      0.71      1446\n",
      "        0.0       0.42      0.25      0.32       840\n",
      "        1.0       0.67      0.22      0.33       537\n",
      "\n",
      "avg / total       0.56      0.57      0.52      2823\n",
      "\n",
      "Average f1-score =  0.429504502873\n",
      "Overall Accuracy =  56.6749550939 %\n",
      "positive f1-score =  0.301603130773\n",
      "negative f1-score =  0.709483888945\n",
      "positive precision =  0.621809823209\n",
      "negative precision =  0.580400716345\n",
      "positive recall =  0.199085515704\n",
      "negative recall =  0.91460942227\n"
     ]
    }
   ],
   "source": [
    "#k fold cross validaiton\n",
    "avg_acc = []\n",
    "avg_f1 = []\n",
    "f_pos = []\n",
    "f_neg = []\n",
    "precision_pos = []\n",
    "precision_neg = []\n",
    "recall_pos = []\n",
    "recall_neg = []\n",
    "\n",
    "#data = data[0:500]\n",
    "# kf = KFold(n=len(data),n_folds=10)\n",
    "kf = StratifiedKFold(Y,n_folds=2)\n",
    "for train,test in kf: #do the cross validation\n",
    "    np.random.seed(1)\n",
    "    x_train, x_val, y_train, y_val = data[train], data[test], labels[train], labels[test]\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(x_train, y_train, nb_epoch=15, batch_size=64,verbose=0) #ep = 20 .5979\n",
    "    y_true,y_pred = get_Ytrue_Ypred(model,x_val,y_val)\n",
    "    avg_acc.append(accuracy_score(y_true,y_pred))\n",
    "    avg_f1.append(f1_score(y_true,y_pred,average='macro'))      \n",
    "    print classification_report(y_true,y_pred)\n",
    "    precision, recall, fscore, support = score(y_true, y_pred)\n",
    "    f_pos.append(fscore[2])\n",
    "    f_neg.append(fscore[0])\n",
    "    precision_pos.append(precision[2])\n",
    "    precision_neg.append(precision[0])\n",
    "    recall_pos.append(recall[2])\n",
    "    recall_neg.append(recall[0])\n",
    "    \n",
    "\n",
    "\n",
    "#print classification_report(y_true,y_pred)\n",
    "print 'Average f1-score = ', np.mean(np.array(avg_f1))\n",
    "print 'Overall Accuracy = ',100.0*np.mean(np.array(avg_acc)),'%'\n",
    "print 'positive f1-score = ', np.mean(np.array(f_pos))\n",
    "print 'negative f1-score = ', np.mean(np.array(f_neg))\n",
    "print 'positive precision = ', np.mean(np.array(precision_pos))\n",
    "print 'negative precision = ', np.mean(np.array(precision_neg))\n",
    "print 'positive recall = ', np.mean(np.array(recall_pos))\n",
    "print 'negative recall = ', np.mean(np.array(recall_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
