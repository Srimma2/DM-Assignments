{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import sys,os,string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(data,mode = 'train'):\n",
    "    \"\"\" get and clean the data \"\"\"\n",
    "    data = data.iloc[1:]\n",
    "    data['text'] = data['text'].values.astype('unicode')\n",
    "    # remove rows with mixed sentiment\n",
    "    data = data[data['sentiment'] < 2]\n",
    "    data.index = range(len(data))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticon Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoticon_dictionary = {':)':' smileyface ','(:':' smileyface ','XD': ' happyface ',':D': ' smileyface ','>.<':' smileyface ',':-)':' smileyface ',';)':' winkface ',';D':' winkface ',':\\'(':' cryingface '}\n",
    "\n",
    "emoticons = [':\\)','\\(:','XD',':D','>\\.<',':-\\)',';\\)',';D',':\\'\\(']\n",
    "\n",
    "emoticon_pattern = re.compile(r'(' + '\\s*|\\s*'.join(emoticons) + r')')\n",
    "\n",
    "# convert emoticons to words\n",
    "def emoticon_converter(x):\n",
    "    x = emoticon_pattern.sub(lambda i : emoticon_dictionary[i.group().replace(' ','')],x)   \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hashTagSplit import *\n",
    "\n",
    "def separate_hashtag(x):\n",
    "    x = x.split()\n",
    "    temp = []\n",
    "    for i,word in enumerate(x):\n",
    "        if '#' in word:\n",
    "            if any(w.isupper() for w in word):\n",
    "                temp += re.findall('[A-Z][^A-Z]*',word)\n",
    "            else:\n",
    "                if len(word) > 1:\n",
    "                    temp += [split_hashtag(word[1:])]\n",
    "        else:\n",
    "            temp.append(word)\n",
    "    \n",
    "    return ' '.join(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "punc = ['\\:','\\;','\\?','\\$','\\.','\\(','\\)','\\=','\\%','\\-','\\>','\\<','\\,','\\\"','\\\\','\\&','\\+']\n",
    "cond_1 = re.compile('|'.join(punc))\n",
    "# remove tags\n",
    "tags = ['<a>','</a>','<e>','</e>']\n",
    "cond_2 = re.compile(\"|\".join(tags))\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\" preprocess the data\"\"\"\n",
    "     # remove users\n",
    "    data = data.apply(lambda x : re.sub(r'\\@\\s?\\w+','',x))\n",
    "    # remove hypertext \n",
    "    data = data.apply(lambda x : re.sub(r'http://\\S+','',x))\n",
    "    # remove tags\n",
    "    data = data.apply(lambda x : re.sub(cond_2,'',x))\n",
    "    # remove punctuations\n",
    "    data = data.apply(lambda x : re.sub(cond_1,'',x))\n",
    "    # remove digits\n",
    "    data = data.apply(lambda x : re.sub(r'[0-9]+','',x))\n",
    "    # convert to ascii\n",
    "    data = data.apply(lambda x: x.encode('utf-8'))\n",
    "    printable = set(string.printable)\n",
    "    for i in range(len(data)):\n",
    "        data[i] = filter(lambda x: x in printable, data[i])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "manual_stopwords_list = ['RT','MT']\n",
    "stopwords_list = stopwords.words('english') + manual_stopwords_list\n",
    "\n",
    "\n",
    "# stopwords list based on pos tags\n",
    "remove_tags_nltkpos = ['IN','DT','PRP','CC']\n",
    "\n",
    "\n",
    "def pos_tag_filter(x):\n",
    "    x = x.split()\n",
    "    s = nltk.pos_tag(x)\n",
    "    for i,(_,tag) in enumerate(s):\n",
    "        if tag in remove_tags_nltkpos:\n",
    "            x[i] = ''\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "class WordTokenizer(object):\n",
    "    def __init__(self,stemmer='porter'):\n",
    "        self.stemmer = stemmer\n",
    "        if stemmer == 'wordnet':\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        if stemmer == 'porter':\n",
    "            self.wnl = PorterStemmer()\n",
    "        if stemmer == 'snowball':\n",
    "            self.wnl = SnowballStemmer('english')\n",
    "    def __call__(self,doc):\n",
    "        if self.stemmer == 'wordnet':\n",
    "            return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "        else:\n",
    "            return [self.wnl.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_FILE = 'glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "EMBEDDING_DIM = 200 #size of word vector \n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC,libsvm,SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score,classification_report,accuracy_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "def get_X_y(data):\n",
    "    return data['text'],data['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "\n",
    "def model_pipeline(X,WordTokenizer,text_vector = None, svd_transform = None,mode = 'train'):\n",
    "\n",
    "    if mode == 'train':\n",
    "        text_vector = Pipeline([('vect', CountVectorizer(tokenizer = WordTokenizer('wordnet'),stop_words = [],ngram_range = (1,2),max_features=10000)),\n",
    "                    ('tfidf',TfidfTransformer())])\n",
    "        svd_transform = TruncatedSVD(n_components = 1000,n_iter = 5)\n",
    "        # transform the data\n",
    "        X = text_vector.fit_transform(X)\n",
    "        X_reduced = svd_transform.fit_transform(X)\n",
    "        return X,X_reduced,text_vector,svd_transform\n",
    "    else:\n",
    "        X = text_vector.transform(X)\n",
    "        X_reduced = svd_transform.transform(X)\n",
    "        return X,X_reduced  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_train(X,y,clfname = 'NaiveBayes'):\n",
    "    if clfname == 'NaiveBayes':\n",
    "        clf = MultinomialNB()\n",
    "    else:\n",
    "        clf = SVC(kernel = 'linear',probability=True)\n",
    "    clf = clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "def classifier_predict(clf,X):\n",
    "    return clf.predict_proba(X)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import optimizers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30 #max number of sentences in a message\n",
    "MAX_NB_WORDS = 20000 #cap vocabulary\n",
    "TOKENIZER = 'keras' #or use nltk\n",
    "STEMMER = 'wordnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Ytrue_Ypred(model,x,y):\n",
    "    #Y matrix is [1,0,0] for class 0, [0,1,0] for class 1, [0,0,1] for class -1\n",
    "    convert_to_label ={0:0,1:1,2:-1}\n",
    "    model_predictions = model.predict(x)\n",
    "    y_pred = np.zeros(len(y))\n",
    "    y_true = np.zeros(len(y))\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        y_pred[i] = convert_to_label[np.argmax(model_predictions[i])]\n",
    "        y_true[i] = convert_to_label[np.argmax(y[i])]\n",
    "\n",
    "    return y_true,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class weighted_categorical_crossentropy(object):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        loss = weighted_categorical_crossentropy(weights).loss\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,weights):\n",
    "        self.weights = K.variable(weights)\n",
    "        \n",
    "    def loss(self,y_true, y_pred):\n",
    "        # scale preds so that the class probas of each sample sum to 1\n",
    "        y_pred /= y_pred.sum(axis=-1, keepdims=True)\n",
    "        # clip\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "        # calc\n",
    "        loss = y_true*K.log(y_pred)*self.weights\n",
    "        loss =-K.sum(loss,-1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kerasprocess_data(texts,labels = None,tokenizer = None,mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(texts) #list of lists, basically replaces each word with number\n",
    "\n",
    "    #pad the data \n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        #prepare embedding matrix\n",
    "        num_words = len(word_index)+1\n",
    "        embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "        return data,labels,embedding_matrix,tokenizer\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GRU_train(data,labels,embedding_matrix,data_name='Obama'):\n",
    "    labels = keras.utils.np_utils.to_categorical(labels,nb_classes=3)\n",
    "    \n",
    "    if data_name == 'Obama':\n",
    "        clf = obama_build_model(embedding_matrix,3)\n",
    "    else:\n",
    "        clf = romney_build_model(embedding_matrix,3)\n",
    "    clf.fit(data,labels, nb_epoch=50, batch_size=64,verbose=0)\n",
    "    return clf\n",
    "    \n",
    "\n",
    "def GRU_predict(clf,data):\n",
    "    predict_probs = clf.predict(data)\n",
    "    # keras predicts probabilites on 0,1,-1 should be -1,0,1\n",
    "    predict_probs[:,[0,1,2]] = predict_probs[:,[2,0,1]]\n",
    "    return predict_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obama data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def obama_build_model(embedding_matrix,labels_len):\n",
    "    np.random.seed(1)\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    l2 = regularizers.l2(0.01)\n",
    "    l22 = regularizers.l2(0.01)\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=0)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(GRU(100,return_sequences=False,dropout_W=0.6,dropout_U=0.5))\n",
    "    weights = np.array([1,2,1]) #index 0 for class 0, index 1 for class 1, index 2 for class -1\n",
    "    mloss = weighted_categorical_crossentropy(weights).loss\n",
    "    sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    model.add(Dense(labels_len, activation='softmax'))\n",
    "    model.compile(loss=mloss, optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Romney data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the hyperparameters\n",
    "def romney_build_model(embedding_matrix,labels_len):\n",
    "    np.random.seed(1)\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    l2 = regularizers.l2(0.01)\n",
    "    l22 = regularizers.l2(0.01)\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=0)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(GRU(100,return_sequences=False,dropout_W=0.6,dropout_U=0.5))\n",
    "    weights = np.array([1,2,1]) #index 0 for class 0, index 1 for class 1, index 2 for class -1\n",
    "    mloss = weighted_categorical_crossentropy(weights).loss\n",
    "    sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    model.add(Dense(labels_len, activation='softmax'))\n",
    "    model.compile(loss=mloss, optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obama_fullcommonpipeline(filename,mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        obama_data = pd.read_excel(filename,names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Obama')\n",
    "    else:\n",
    "        obama_data = pd.read_excel(testfilename,sheetname = 'Obama')\n",
    "        obama_data['text'] = obama_data['Anootated tweet']\n",
    "        obama_data['sentiment'] = obama_data['Unnamed: 4']\n",
    "\n",
    "    obama_data = get_data(obama_data,mode)\n",
    "    obama_data['text'] = obama_data['text'].apply(emoticon_converter)\n",
    "    obama_data['text'] = obama_data['text'].apply(separate_hashtag)\n",
    "    obama_data['text'] = preprocess(obama_data['text'])\n",
    "    obama_data['text'] = obama_data['text'].apply(pos_tag_filter)\n",
    "    obama_data['text'] = obama_data['text'].apply(lambda x : x.lower())\n",
    "    return obama_data\n",
    "\n",
    "def obama_fulltrainpipeline(trainfilename):\n",
    "    obama_data = obama_fullcommonpipeline(trainfilename)\n",
    "    X,y = get_X_y(obama_data)\n",
    "    X,X_reduced,text_vector,svd_transform = model_pipeline(X,WordTokenizer)\n",
    "    bayes_clf = classifier_train(X,y)\n",
    "    svm_clf = classifier_train(X_reduced,y,clfname = 'LinearSVM')\n",
    "    \n",
    "    texts = obama_data['text']\n",
    "    labels = np.array(obama_data['sentiment'])\n",
    "\n",
    "    data,labels,embedding_matrix,tokenizer = kerasprocess_data(texts,labels)\n",
    "    gru_clf = GRU_train(data,labels,embedding_matrix)\n",
    "    \n",
    "    bookkeep = dict()\n",
    "    bookkeep['text_vector'] = text_vector\n",
    "    bookkeep['svd_transform'] = svd_transform\n",
    "    bookkeep['tokenizer'] = tokenizer\n",
    "    \n",
    "    return bayes_clf,svm_clf,gru_clf,bookkeep\n",
    "\n",
    "def obama_fullpredictpipeline(trainfilename,testfilename,q = None):\n",
    "    obama_data = obama_fullcommonpipeline(testfilename,mode = 'test')\n",
    "    bayes_clf,svm_clf,gru_clf,bookkeep = obama_fulltrainpipeline(trainfilename)\n",
    "    X = obama_data['text']\n",
    "    X,X_reduced = model_pipeline(X,WordTokenizer,text_vector = bookkeep['text_vector'],svd_transform = bookkeep['svd_transform'],mode = 'test')\n",
    "    bayes_pred = classifier_predict(bayes_clf,X)\n",
    "    svm_pred = classifier_predict(svm_clf,X_reduced)\n",
    "    \n",
    "    texts = obama_data['text']\n",
    "\n",
    "    data = kerasprocess_data(texts,tokenizer = bookkeep['tokenizer'],mode = 'test')\n",
    "    gru_pred = GRU_predict(gru_clf,data)\n",
    "\n",
    "    ensemble_pred = (bayes_pred + svm_pred + gru_pred)/3\n",
    "    \n",
    "    if q:\n",
    "        q.put(('obama',np.argmax(ensemble_pred,axis = 1) - 1))\n",
    "        return\n",
    "\n",
    "    return np.argmax(ensemble_pred,axis = 1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def romney_fullcommonpipeline(filename,mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        romney_data = pd.read_excel(filename,names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Romney')\n",
    "    else:\n",
    "        romney_data = pd.read_excel(testfilename,sheetname = 'Romney')\n",
    "        romney_data['text'] = romney_data['Anootated tweet']\n",
    "        romney_data['sentiment'] = romney_data['Unnamed: 4']\n",
    "    romney_data = get_data(romney_data,mode)\n",
    "    romney_data['text'] = romney_data['text'].apply(emoticon_converter)\n",
    "    romney_data['text'] = romney_data['text'].apply(separate_hashtag)\n",
    "    romney_data['text'] = preprocess(romney_data['text'])\n",
    "    romney_data['text'] = romney_data['text'].apply(pos_tag_filter)\n",
    "    romney_data['text'] = romney_data['text'].apply(lambda x : x.lower())\n",
    "    return romney_data\n",
    "\n",
    "def romney_fulltrainpipeline(trainfilename):\n",
    "    romney_data = romney_fullcommonpipeline(trainfilename)\n",
    "    X,y = get_X_y(romney_data)\n",
    "    _,X_reduced,text_vector,svd_transform = model_pipeline(X,WordTokenizer)\n",
    "    svm_clf = classifier_train(X_reduced,y,clfname = 'LinearSVM')\n",
    "    \n",
    "    texts = romney_data['text']\n",
    "    labels = np.array(romney_data['sentiment'])\n",
    "\n",
    "    data,labels,embedding_matrix,tokenizer = kerasprocess_data(texts,labels)\n",
    "    gru_clf = GRU_train(data,labels,embedding_matrix,data_name = 'Romney')\n",
    "    \n",
    "    bookkeep = dict()\n",
    "    bookkeep['text_vector'] = text_vector\n",
    "    bookkeep['svd_transform'] = svd_transform\n",
    "    bookkeep['tokenizer'] = tokenizer\n",
    "    \n",
    "    return svm_clf,gru_clf,bookkeep\n",
    "\n",
    "def romney_fullpredictpipeline(trainfilename,testfilename,q = None):\n",
    "    romney_data = romney_fullcommonpipeline(testfilename,mode = 'test')\n",
    "    svm_clf,gru_clf,bookkeep = romney_fulltrainpipeline(trainfilename)\n",
    "    X = romney_data['text']\n",
    "    X,X_reduced = model_pipeline(X,WordTokenizer,text_vector = bookkeep['text_vector'],svd_transform = bookkeep['svd_transform'],mode = 'test')\n",
    "    svm_pred = classifier_predict(svm_clf,X_reduced)\n",
    "    \n",
    "    texts = romney_data['text']\n",
    "\n",
    "    data = kerasprocess_data(texts,tokenizer = bookkeep['tokenizer'],mode = 'test')\n",
    "    gru_pred = GRU_predict(gru_clf,data)\n",
    "    \n",
    "    ensemble_pred = (gru_pred + svm_pred)/2\n",
    "    \n",
    "    if q:\n",
    "        q.put(('romney',np.argmax(ensemble_pred,axis = 1) - 1))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    return np.argmax(ensemble_pred,axis = 1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainfilename = 'training-Obama-Romney-tweets.xlsx'\n",
    "testfilename = 'testing-Obama-Romney-tweets.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process,Queue\n",
    "\n",
    "q = Queue()\n",
    "p1 = Process(target = obama_fullpredictpipeline, args = (trainfilename,testfilename,q))\n",
    "p1.start()\n",
    "p2 = Process(target = romney_fullpredictpipeline, args = (trainfilename,testfilename,q))\n",
    "p2.start()\n",
    "p1.join()\n",
    "p2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_val = dict()\n",
    "while not q.empty():\n",
    "    i = q.get()\n",
    "    if i[0] == 'obama':\n",
    "        pred_val['obama'] = i[1]\n",
    "    else:\n",
    "        pred_val['romney'] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "obama_data = pd.read_excel(testfilename,sheetname = 'Obama')\n",
    "obama_data['text'] = obama_data['Anootated tweet']\n",
    "obama_data['sentiment'] = obama_data['Unnamed: 4']\n",
    "obama_data = get_data(obama_data,mode = 'test')\n",
    "obama_true_val = obama_data['sentiment'].astype(int).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "romney_data = pd.read_excel(testfilename,sheetname = 'Romney')\n",
    "romney_data['text'] = romney_data['Anootated tweet']\n",
    "romney_data['sentiment'] = romney_data['Unnamed: 4']\n",
    "romney_data = get_data(romney_data,mode = 'test')\n",
    "romney_true_val = romney_data['sentiment'].astype(int).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama report\n",
      "Overall Accuracy is 0.594566888775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1    0.58343   0.72674   0.64725       688\n",
      "          0    0.58748   0.46843   0.52124       681\n",
      "          1    0.61887   0.58591   0.60194       582\n",
      "\n",
      "avg / total    0.59542   0.59457   0.58975      1951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Obama report'\n",
    "print 'Overall Accuracy is {}'.format(accuracy_score(obama_true_val,pred_val['obama']))\n",
    "print classification_report(obama_true_val,pred_val['obama'],digits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romney report\n",
      "Overall Accuracy is 0.635789473684\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1    0.66723   0.82917   0.73943       960\n",
      "          0    0.57803   0.36036   0.44395       555\n",
      "          1    0.58726   0.55065   0.56836       385\n",
      "\n",
      "avg / total    0.62497   0.63579   0.61846      1900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Romney report'\n",
    "print 'Overall Accuracy is {}'.format(accuracy_score(romney_true_val,pred_val['romney']))\n",
    "print classification_report(romney_true_val,pred_val['romney'],digits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
