{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "TWITTER_FILE = 'training-Obama-Romney-tweets.xlsx'\n",
    "\n",
    "# read the data\n",
    "obama_data = pd.read_excel(TWITTER_FILE,names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Obama')\n",
    "romney_data = pd.read_excel(TWITTER_FILE,names = ['date','time','text','sentiment'],parse_cols = 4,sheetname = 'Romney')\n",
    "\n",
    "def get_data(data):\n",
    "    \"\"\" get and clean the data \"\"\"\n",
    "    data = data.iloc[1:]\n",
    "    data['text'] = data['text'].values.astype('unicode')\n",
    "    data['date'] = data['date'].values.astype('str')\n",
    "    data['time'] = data['time'].values.astype('unicode')\n",
    "    # remove rows with mixed sentiment\n",
    "    data = data[data['sentiment'] < 2]\n",
    "    data.index = range(len(data))\n",
    "    \n",
    "    return data\n",
    "\n",
    "obama_data = get_data(obama_data)\n",
    "romney_data = get_data(romney_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticon Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoticon_dictionary = {':)':' smileyface ','(:':' smileyface ','XD': ' happyface ',':D': ' smileyface ','>.<':' smileyface ',':-)':' smileyface ',';)':' winkface ',';D':' winkface ',':\\'(':' cryingface '}\n",
    "\n",
    "emoticons = [':\\)','\\(:','XD',':D','>\\.<',':-\\)',';\\)',';D',':\\'\\(']\n",
    "\n",
    "emoticon_pattern = re.compile(r'(' + '\\s*|\\s*'.join(emoticons) + r')')\n",
    "\n",
    "# convert emoticons to words\n",
    "def emoticon_converter(x):\n",
    "    x = emoticon_pattern.sub(lambda i : emoticon_dictionary[i.group().replace(' ','')],x)   \n",
    "    return x\n",
    "\n",
    "obama_data['text'] = obama_data['text'].apply(emoticon_converter)\n",
    "romney_data['text'] = romney_data['text'].apply(emoticon_converter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_hashtag(x):\n",
    "    x = x.split()\n",
    "    temp = []\n",
    "    for i,word in enumerate(x):\n",
    "        if '#' in word:\n",
    "            if any(w.isupper for w in word):\n",
    "                temp += re.findall('[A-Z][^A-Z]*',word)\n",
    "            else:\n",
    "                # Should add code\n",
    "                pass\n",
    "        else:\n",
    "            temp.append(word)\n",
    "    \n",
    "    return ' '.join(temp)\n",
    "\n",
    "obama_data['text'] = obama_data['text'].apply(separate_hashtag)\n",
    "romney_data['text'] = romney_data['text'].apply(separate_hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "punc = ['\\:','\\;','\\?','\\$','\\.','\\(','\\)','\\#','\\=','\\%','\\-','\\>','\\<','\\,','\\\"','\\\\','\\&']\n",
    "cond_1 = re.compile('|'.join(punc))\n",
    "# remove tags\n",
    "tags = ['<a>','</a>','<e>','</e>']\n",
    "cond_2 = re.compile(\"|\".join(tags))\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\" preprocess the data\"\"\"\n",
    "     # remove users\n",
    "    data = data.apply(lambda x : re.sub(r'\\@\\s?\\w+','',x))\n",
    "    # remove hypertext \n",
    "    data = data.apply(lambda x : re.sub(r'http://\\S+','',x))\n",
    "    # remove tags\n",
    "    data = data.apply(lambda x : re.sub(cond_2,'',x))\n",
    "    # remove punctuations\n",
    "    data = data.apply(lambda x : re.sub(cond_1,'',x))\n",
    "    # remove digits\n",
    "    data = data.apply(lambda x : re.sub(r'[0-9]+','',x))\n",
    "    # convert to ascii\n",
    "    data = data.apply(lambda x: x.encode('utf-8'))\n",
    "    \n",
    "    return data\n",
    "\n",
    "obama_data['text'] = preprocess(obama_data['text'])\n",
    "romney_data['text'] = preprocess(romney_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "manual_stopwords_list = ['RT','MT']\n",
    "stopwords_list = stopwords.words('english') + manual_stopwords_list\n",
    "\n",
    "\n",
    "# stopwords list based on pos tags\n",
    "\n",
    "remove_tags_nltkpos = ['IN','DT','PRP','CC']\n",
    "\n",
    "\n",
    "def pos_tag_filter(x):\n",
    "    x = x.split()\n",
    "    s = nltk.pos_tag(x)\n",
    "    for i,(_,tag) in enumerate(s):\n",
    "        if tag in remove_tags_nltkpos:\n",
    "            x[i] = ''\n",
    "    return ' '.join(x)\n",
    "    \n",
    "\n",
    "# obama_data['text'] = obama_data['text'].apply(pos_tag_filter)\n",
    "romney_data['text'] = romney_data['text'].apply(pos_tag_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to be checked \n",
    "obama_data['text'] = obama_data['text'].apply(lambda x : x.lower())\n",
    "romney_data['text'] = romney_data['text'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "class WordTokenizer(object):\n",
    "    def __init__(self,stemmer='porter'):\n",
    "        self.stemmer = stemmer\n",
    "        if stemmer == 'wordnet':\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        if stemmer == 'porter':\n",
    "            self.wnl = PorterStemmer()\n",
    "        if stemmer == 'snowball':\n",
    "            self.wnl = SnowballStemmer('english')\n",
    "    def __call__(self,doc):\n",
    "        if self.stemmer == 'wordnet':\n",
    "            return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "        else:\n",
    "            return [self.wnl.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_FILE = 'glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "EMBEDDING_DIM = 200 #size of word vector \n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC,libsvm,SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "def get_X_y(data):\n",
    "    return data['text'],data['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "\n",
    "def model_pipeline(X):\n",
    "    \n",
    "    global WordTokenizer\n",
    "\n",
    "    text_vector = Pipeline([('vect', CountVectorizer(tokenizer = WordTokenizer('wordnet'),stop_words = [],ngram_range = (1,2),max_features=10000)),\n",
    "                    ('tfidf',TfidfTransformer())])\n",
    "    svd_transform = TruncatedSVD(n_components = 1000,n_iter = 5)\n",
    "    \n",
    "    # transform the data\n",
    "    X = text_vector.fit_transform(X)\n",
    "    X_reduced = svd_transform.fit_transform(X)\n",
    "    \n",
    "    return X,X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
    "\n",
    "def classifiers_validate(X,X_reduced,y):\n",
    "\n",
    "    classifier_scores = dict()\n",
    "\n",
    "    def naive_classifier():\n",
    "        return 'Naive_Bayes',MultinomialNB()\n",
    "\n",
    "    def svm_classifier():\n",
    "        return 'Linear_SVM',SVC(kernel = 'linear')\n",
    "\n",
    "    classifiers_list = [naive_classifier(),svm_classifier()]\n",
    "\n",
    "    for clf_name,clf in classifiers_list:\n",
    "        # dont use reduced matrix for naive bayes\n",
    "        if clf_name != 'Naive_Bayes':\n",
    "                X = X_reduced\n",
    "        classifier_scores[clf_name] = dict()\n",
    "        classifier_scores[clf_name]['classification_pred'] = cross_val_predict(clf,X,y,cv = 10)\n",
    "        \n",
    "    for clf_name,_ in classifiers_list:\n",
    "        print 'Classifier - {}'.format(clf_name)\n",
    "        print 'accuracy is {}'.format(accuracy_score(y,classifier_scores[clf_name]['classification_pred']))\n",
    "        print classification_report(y,classifier_scores[clf_name]['classification_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_classify(X,y,clfname = 'NaiveBayes'):\n",
    "    if clfname == 'NaiveBayes':\n",
    "        clf = MultinomialNB()\n",
    "    else:\n",
    "        clf = SVC(kernel = 'linear',probability=True)\n",
    "    clf = clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "def classifier_predict(clf,X):\n",
    "    return clf.predict_proba(X)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import optimizers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30 #max number of sentences in a message\n",
    "MAX_NB_WORDS = 20000 #cap vocabulary\n",
    "TOKENIZER = 'keras' #or use nltk\n",
    "STEMMER = 'wordnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Ytrue_Ypred(model,x,y):\n",
    "    #Y matrix is [1,0,0] for class 0, [0,1,0] for class 1, [0,0,1] for class -1\n",
    "    convert_to_label ={0:0,1:1,2:-1}\n",
    "    model_predictions = model.predict(x)\n",
    "    y_pred = np.zeros(len(y))\n",
    "    y_true = np.zeros(len(y))\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        y_pred[i] = convert_to_label[np.argmax(model_predictions[i])]\n",
    "        y_true[i] = convert_to_label[np.argmax(y[i])]\n",
    "\n",
    "    return y_true,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class weighted_categorical_crossentropy(object):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        loss = weighted_categorical_crossentropy(weights).loss\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,weights):\n",
    "        self.weights = K.variable(weights)\n",
    "        \n",
    "    def loss(self,y_true, y_pred):\n",
    "        # scale preds so that the class probas of each sample sum to 1\n",
    "        y_pred /= y_pred.sum(axis=-1, keepdims=True)\n",
    "        # clip\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "        # calc\n",
    "        loss = y_true*K.log(y_pred)*self.weights\n",
    "        loss =-K.sum(loss,-1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kerasprocess_data(texts,labels):\n",
    "\n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts) #list of lists, basically replaces each word with number\n",
    "\n",
    "    tokens = []\n",
    "  \n",
    "    myTokenizer = WordTokenizer(STEMMER)\n",
    "        \n",
    "    for i in range(0,len(texts)):\n",
    "        try:\n",
    "            tokens.append(myTokenizer.__call__(texts[i]))\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    word_dict = {}\n",
    "    winx = 1\n",
    "    mysequences = []\n",
    "    tsq = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        for token in tokens[i]:\n",
    "            if token not in word_dict:\n",
    "                word_dict[token] = winx\n",
    "                winx += 1\n",
    "            tsq.append(word_dict[token])\n",
    "        mysequences.append(tsq)\n",
    "        tsq = []\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    if TOKENIZER == 'nltk':\n",
    "        word_index = word_dict\n",
    "        sequences = mysequences\n",
    "\n",
    "    #pad the data \n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    Y = labels\n",
    "    \n",
    "    \n",
    "    #prepare embedding matrix\n",
    "\n",
    "    num_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return data,Y,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GRU_validate(data,Y,embedding_matrix,data_name = 'Obama'):\n",
    "\n",
    "    #k fold cross validaiton\n",
    "    avg_acc = []\n",
    "    avg_f1 = []\n",
    "    f_pos = []\n",
    "    f_neg = []\n",
    "    precision_pos = []\n",
    "    precision_neg = []\n",
    "    recall_pos = []\n",
    "    recall_neg = []\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=2)\n",
    "    labels = keras.utils.np_utils.to_categorical(Y,nb_classes=3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for train,test in kf.split(data,Y): #do the cross validation\n",
    "        np.random.seed(1)\n",
    "        x_train, x_val, y_train, y_val = data[train], data[test], labels[train], labels[test]\n",
    "\n",
    "        if data_name == 'Obama':\n",
    "            model = obama_build_model(embedding_matrix,len(labels[0]))\n",
    "        else:\n",
    "            model = romney_build_model(embedding_matrix,len(labels[0]))\n",
    "        \n",
    "        model.fit(x_train, y_train, nb_epoch=15, batch_size=64,verbose=0) #ep = 20 .5979\n",
    "        y_true,y_pred = get_Ytrue_Ypred(model,x_val,y_val)\n",
    "        avg_acc.append(accuracy_score(y_true,y_pred))\n",
    "        avg_f1.append(f1_score(y_true,y_pred,average='macro'))      \n",
    "        print classification_report(y_true,y_pred)\n",
    "        precision, recall, fscore, support = score(y_true, y_pred)\n",
    "        f_pos.append(fscore[2])\n",
    "        f_neg.append(fscore[0])\n",
    "        precision_pos.append(precision[2])\n",
    "        precision_neg.append(precision[0])\n",
    "        recall_pos.append(recall[2])\n",
    "        recall_neg.append(recall[0])\n",
    "    \n",
    "    #print classification_report(y_true,y_pred)\n",
    "    print 'Average f1-score = ', np.mean(np.array(avg_f1))\n",
    "    print 'Overall Accuracy = ',100.0*np.mean(np.array(avg_acc)),'%'\n",
    "    print 'positive f1-score = ', np.mean(np.array(f_pos))\n",
    "    print 'negative f1-score = ', np.mean(np.array(f_neg))\n",
    "    print 'positive precision = ', np.mean(np.array(precision_pos))\n",
    "    print 'negative precision = ', np.mean(np.array(precision_neg))\n",
    "    print 'positive recall = ', np.mean(np.array(recall_pos))\n",
    "    print 'negative recall = ', np.mean(np.array(recall_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement this\n",
    "def GRU_classify(data,labels,embedding_matrix):\n",
    "    pass\n",
    "\n",
    "def GRU_predict(clf,data,embedding_matrix):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obama data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def obama_build_model(embedding_matrix,labels_len):\n",
    "    np.random.seed(1)\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    l2 = regularizers.l2(0.01)\n",
    "    l22 = regularizers.l2(0.01)\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=0)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(GRU(10,return_sequences=False,dropout_W=0.6,dropout_U=0.5))\n",
    "    weights = np.array([1,2,1]) #index 0 for class 0, index 1 for class 1, index 2 for class -1\n",
    "    mloss = weighted_categorical_crossentropy(weights).loss\n",
    "    sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    model.add(Dense(labels_len, activation='softmax'))\n",
    "    model.compile(loss=mloss, optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier - Naive_Bayes\n",
      "accuracy is 0.592761835131\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.56      0.72      0.63      1922\n",
      "          0       0.57      0.49      0.53      1896\n",
      "          1       0.68      0.56      0.61      1653\n",
      "\n",
      "avg / total       0.60      0.59      0.59      5471\n",
      "\n",
      "Classifier - Linear_SVM\n",
      "accuracy is 0.583805520015\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.58      0.66      0.62      1922\n",
      "          0       0.54      0.55      0.54      1896\n",
      "          1       0.66      0.54      0.59      1653\n",
      "\n",
      "avg / total       0.59      0.58      0.58      5471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes and Linear SVM\n",
    "X,y = get_X_y(obama_data)\n",
    "X,X_reduced = model_pipeline(X)\n",
    "classifiers_validate(X,X_reduced,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.53      0.47      0.50       961\n",
      "        0.0       0.47      0.22      0.30       948\n",
      "        1.0       0.42      0.72      0.53       827\n",
      "\n",
      "avg / total       0.48      0.46      0.44      2736\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.60      0.38      0.46       961\n",
      "        0.0       0.57      0.16      0.25       948\n",
      "        1.0       0.39      0.87      0.53       826\n",
      "\n",
      "avg / total       0.52      0.45      0.41      2735\n",
      "\n",
      "Average f1-score =  0.429889234747\n",
      "Overall Accuracy =  45.6039922704 %\n",
      "positive f1-score =  0.531779361716\n",
      "negative f1-score =  0.480238317736\n",
      "positive precision =  0.401476947068\n",
      "negative precision =  0.564364219441\n",
      "positive recall =  0.796172460335\n",
      "negative recall =  0.422996878252\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "texts = obama_data['text']\n",
    "labels = np.array(obama_data['sentiment'])\n",
    "\n",
    "data,labels,embedding_matrix = kerasprocess_data(texts,labels)\n",
    "GRU_validate(data,labels,embedding_matrix,data_name = 'Obama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Romney data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the hyperparameters\n",
    "def romney_build_model(embedding_matrix,labels_len):\n",
    "    np.random.seed(1)\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    l2 = regularizers.l2(0.01)\n",
    "    l22 = regularizers.l2(0.01)\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=0)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(GRU(10,return_sequences=False,dropout_W=0.6,dropout_U=0.5))\n",
    "    weights = np.array([1,2,1]) #index 0 for class 0, index 1 for class 1, index 2 for class -1\n",
    "    mloss = weighted_categorical_crossentropy(weights).loss\n",
    "    sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "    model.add(Dense(labels_len, activation='softmax'))\n",
    "    model.compile(loss=mloss, optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Naive bayes and SVM\n",
    "X,y = get_X_y(romney_data)\n",
    "X,X_reduced = model_pipeline(X)\n",
    "classifiers_validate(X,X_reduced,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRU\n",
    "texts = romney_data['text']\n",
    "labels = np.array(romney_data['sentiment'])\n",
    "\n",
    "data,labels,embedding_matrix = kerasprocess_data(texts,labels)\n",
    "GRU_validate(data,labels,embedding_matrix,data_name = 'Romney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
